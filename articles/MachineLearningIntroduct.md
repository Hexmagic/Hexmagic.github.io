> 机器学习是对能通过经验自动改进的计算机算法的研究。 — Mitchell [1997]

通俗地讲，机器学习（Machine Learning，ML）就是让计算机从数据中进行 自动学习，得到某种知识（或规律）。作为一门学科，机器学习通常指一类问题以 及解决这类问题的方法，即如何从观测数据（样本）中寻找规律，并利用学习到的 规律（模型）对未知或无法观测的数据进行预测。 

***

# 1. 基本概念

首先介绍下机器学习中的一些基本概念：包括样本、特征、标签、模型、学习 算法等。以一个生活中的经验学习为例，假设我们要到市场上购买芒

## 1.1 特征和标签

首先，我们从市场上随机选取一些芒果，列出每个芒果的**特征**（Feature），包
括颜色，大小，形状，产地，品牌，以及我们需要预测的**标签**（Label）。标签可以连 续值（比如关于芒果的甜度、水分以及成熟度的综合打分），也可以是离散值（比 如“好”“坏”两类标签）。 

## 1.2 数据集、训练集、测试集

一个标记好特征以及标签的芒果可以看作是一个样本（Sample）。一组构成的集合称为**数据集**（Data Set）。一般将数据集分为两部分：**训练集**和**测试 集**。训练集（Training Set）中的样本是用来训练模型的，也叫**训练样本**（Training Sample），而**测试集**（Test Set）中的样本是用来检验模型好坏的，也叫测试样本 （Test Sample）。

## 1.3 特征向量

我们用一个d维向量x = [x1,x2,··· ,xd]T 表示一个芒果的所有特征构成的 向量，称为特征向量（Feature Vector），其中每一维表示一个特征

## 1.4 模型

假设训练集由N 个样本组成，其中每个样本都是独立同分布

D={(x<sup>1</sup>,y<sup>1</sup>),(x<sup>2</sup>,y<sup>2</sup>),··· ,(x<sup>N</sup>,y<sup>N</sup>)}

给定训练集D，我们希望让计算机自动寻找一个函数f(x;θ)来建立每个样 本特性向量x和标签y 之间的映射。对于一个样本x，我们可以通过决策函数来 预测其标签的值

![](img/model.png)

或标签的条件概率

![](img/pro.png)

其中θ为可学习的参数。 

通过一个学习算法（Learning Algorithm）A，在训练集上找到一组参数θ∗， 使得函数f(x,θ∗)可以近似真实的映射关系。这个过程称为学习（Learning）或**训 练**（Training）过程，函数f(x;θ)称为**模型**（Model)

***

# 2. 机器学习三要素

## 2.1 模型

一个机器学习任务要先需要确定其输入空间X 和输出空间Y。不同机器学 习任务的主要区别在于输出空间不同。在两类分类问题中Y = {+1,−1}，在C 类分类问题中Y ={1,2,··· ,C}，而在回归问题中Y = R
输入空间X 和输出空间Y 构成了一个样本空间。对于样本空间中的样本 (x,y)∈X ×Y，假定存在一个未知的真实映射函数g :X →Y使得 

![](img/gx.png)

或者真实条件概率分布

![](img/pr.png)

机器学习的目标是找到一个模型来近似真实映射函数g(x)或真实条件概率分布 pr(y|x)

### 2.1.1 线性模型

线性模型的假设空间为一个参数化的线性函数族

![](img/linear.png)

### 2.1.2 非线性模型

广义的非线性模型可以写为多个非线性基函数ϕ(x)的线性组合

![](img/Mlinear.png)

***

## 2.2 学习策略

​ 由于假设空间是模型的集合，而我们要从集合中选择具体的模型，我们就应该考虑选择的指标与依据。策略就是考虑如何选择模型

### 2.2.1 损失函数

损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异。 下面介绍几种常用的损失函数。

**0-1损失函数**: 最直观的损失函数是模型预测的错误率，即0-1损失函数（0-1Loss Function）。 

![](img/01.png)

**平方损失函数**: 平方损失函数（Quadratic Loss Function）经常用在预测标签y为 实数值的任务中

![](img/mse.png)

**交叉熵损失函数**: 交叉熵损失函数（Cross-Entropy Loss Function）一般用于分 类问题。假设样本的标签y ∈{1,···C}为离散的类别，模型f(x;θ)∈[0,1]C 的输 出为类别标签的条件概率分布，即

![](img/cross.png)

并满足

![](img/cs.png)

虽然0-1损失能够客观的评价模型的好坏，但缺点是数学性质不是很好：不 连续且导数为0，难以优化。因此经常用连续可微的损失函数替代。平方损失函数一般不适用于分类问题

### 2.2.2 过拟合和欠拟合

一个好的模型f(x;θ)应当有一个比较小的期望错误，但由于不知道真实的 数据分布和映射函数，实际上无法计算期望风险R(θ;x,y)。给定一个训练集 D={(x<sup>n</sup>,y<sup>n</sup>)}<sup>N</sup><sub>n=1</sub>，我们可以计算的是经验风险（Empirical Risk），即在训练 集上的平均损失

这就是经验风险最小化（Empirical Risk Minimization，ERM）准则

**过拟合**： 给定一个假设空间F，一个假设f 属于F，如果 存在其他的假设f′ 也属于F,使得在训练集上f 的损失比f′ 小，但在 整个样本空间上f′比f 的损失小，那么就说假设f 过度拟合训练数据 [Mitchell, 1997]

过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的。 为了解决过拟合问题，一般在经验风险最小化的基础上再引入参数的正则化 （Regularization），来限制模型能力，使其不要过度地最小化经验风险。这种 准则就是**结构风险最小化（Structure Risk Minimization，SRM）准则**。

和过拟合相反的一个概念是**欠拟合**（Underﬁtting），即模型不能很好地拟合 训练数据，在训练集的错误率比较高。欠拟合一般是由于模型能力不足造成的

![](img/overfit.png)

***

## 2.3 优化算法

在确定了训练集D、假设空间F 以及学习策略后，如何找到最优的模型 f(x,θ∗)就成了一个最优化（Optimization）问题。机器学习的训练过程其实就 是最优化问题的求解过程

### 2.3.1 参数和超参数

参数与超参数 在机器学习中，优化又可以分为参数优化和超参数优化。模型 f(x;θ)中的θ称为模型的参数，可以通过优化算法进行学习。除了可学习的参数 θ之外，还有一类参数是用来定义模型结构或优化策略的，这类参数叫做**超参数** （Hyper-Parameter）。

在贝叶斯方法中，超参数可以理解为参数的参数，即控 制模型参数分布的参数。
常见的超参数包括：***聚类算法中的类别个数、梯度下降法的步长、正则项的 系数、神经网络的层数、支持向量机中的核函数等***。超参数的选取一般都是组合 优化问题，很难通过优化算法来自动学习。因此，超参数优化是机器学习的一个 经验性很强的技术，通常是按照人的经验设定，或者通过搜索的方法对一组超参 数组合进行不断试错调整

### 2.3.2 梯度下降法

在机器学习中，最简单、常用的优化算法就是[梯度下降法](https://www.cnblogs.com/huangyc/p/9801261.html)，即通过迭代的。
法来计算训练集D上风险函数的最小值

### 2.3.3 提前停止

针对梯度下降的优化算法，除了加正则化项之外，还可以通过提前停止(Early Stop)来防 止过拟合

在梯度下降训练的过程中，由于过拟合的原因，在训练样本上收敛的参数， 并不一定在测试集上最优。因此，除了训练集和测试集之外，有时也会使用一 个验证集（Validation Set）来进行模型选择，测试模型在验证集上是否最优。 在每次迭代时，把新得到的模型f(x;θ)在验证集上进行测试，并计算错误率。 如果在验证集上的错误率不再下降，就停止迭代。这种策略叫**提前停止**

### 2.3.4 随机梯度下降

上面所说的梯度下降是整个训练集上风险函数，这种方式称为批量梯度下降法（Batch Gradient Descent，BGD）。批量梯度下降法在每 次迭代时需要计算每个样本上损失函数的梯度并求和。当训练集中的样本数量 N 很大时，空间复杂度比较高，每次迭代的计算开销也很大

随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度

### 2.3.5 批量梯度下降发

随机梯度下降法的一个缺点是无法充分利用计算机的并行 计算能力。小批量梯度下降法（Mini-Batch Gradient Descent）是批量梯度下降 和随机梯度下降的折中。每次迭代时，我们随机选取一小部分训练样本来计算梯 度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率

### 2.3.6 对比

在实际应用中，小批量随机梯度下降方法有收敛快，计算开销小的优点，因 此逐渐成为大规模的机器学习中的主要优化算法

# 3. 偏差和方差

为了避免过拟合，我们经常会在模型的拟合能力和复杂度之间进行权衡。拟 合能力强的模型一般复杂度会比较高，容易导致过拟合。相反，如果限制模型的 复杂度，降低其拟合能力，又可能会导致欠拟合。因此，如何在模型能力和复杂 度之间取得一个较好的平衡对一个机器学习算法来讲十分重要。**偏差-方差分解**（Bias-Variance Decomposition）为我们提供一个很好的分析和指导工具

方差一般会随着训练样本的增加而减少。当样本比较多时，方差比较少，我 们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往 往都比较有限，最优的偏差和最优的方差就无法兼顾
![](img/error.png)

随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而 导致过拟合。以结构错误最小化为例，我们可以调整正则化系数λ来控制模型的 复杂度。当λ变大时，模型复杂度会降低，可以有效地减少方差，避免过拟合，但偏差会上升。当λ过大时，总的期望错误反而会上升。因此，一个好的正则化系数 λ需要在偏差和方差之间取得比较好的平衡。下图给出了机器学习模型的期望 错误、偏差和方差随复杂度的变化情况。最优的模型并不一定是偏差曲线和方差 曲线的交点
![](img/bias.png)

## 3.1 偏差方差分解

**偏差**和**方差分解**给机器学习模型提供了一种分析途径，但在实际操作中难 以直接衡量。一般来说，当一个模型在训练集上的错误率比较高时，说明模型的 拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型。当模型在训练集上的错误率比较低，但验证集上 的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型 复杂度，加大正则化系数，引入先验等方法来缓解。此外，还有一种有效的降低方 差的方法为集成模型，即通过多个高方差模型的平均来降低方差

# 4. 机器学习算法类型

机器学习算法可以按照不同的标准来进行分类。比如按函数f(x;θ)的不同， 机器学习算法可以分为线性模型和非线性模型；按照学习准则的不同，机器学习 算法也可以分为统计方法和非统计方法
但一般来说，我们会按照训练样本提供的信息以及反馈方式的不同，将机器 学习算法分为以下几类

## 4.1 监督学习

如果机器学习的目标是通过建模样本的特征x和标签y 之间的关系： y = f(x;θ)或p(y|x;θ)，并且训练集中每个样本都有标签，那么这类机器学习称 为监督学习（Supervised Learning）。根据标签类型的不同，监督学习又可以分 为回归和分类两类。

1. 回归（Regression）问题中的标签y是连续值（实数或连续整数）， f(x;θ)的 输出也是连续值。 
2. 分类（Classiﬁcation）问题中的标签y 是离散的类别（符号）。在分类问题 中，学习到模型也称为分类器（Classiﬁer）。分类问题根据其类别数量又可 分为两类分类（Binary Classiﬁcation）和多类分类（Multi-class Classiﬁcation）问题。 

## 4.2 无监督学习

无监督学习（Unsupervised Learning，UL）是指从不包含目标标签 的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有聚类、密 度估计、特征学习、降维等。

## 4.3 强化学习

强化学习（Reinforcement Learning，RL）是一类通过交互来学习的机 器学习算法。在强化学习中，智能体根据环境的状态做出一个动作，并得到即时 或延时的奖励。智能体在和环境的交互中不断学习并调整策略，以取得最大化的 期望总回报。
![](img/mclass.png)

# 5 特征学习

在实际应用中，数据的类型多种多样，比如文本、音频、图像、视频等。不同类 型的数据，其原始特征（Raw Features）的空间也不相同。比如一张灰度图像（像 素数量为n）的特征空间为[0,255]n，一个自然语言句子（长度为L）的特征空间 为|V|L，其中V为词表集合。而很多机器学习算法要求是输入的样本特征是数学 上可计算的，因此，在机器学习之前我们需要将这些不同类型的数据转换为向量 表示。

传统的特征学习一般是通过人为地设计一些准则，然后根据这些准则来选 取有效的特征，具体又可以分为两种：特征选择和特征抽取

## 5.1 特征选择

**特征选择**（Feature Selection）是选取原始特征集合的一个有效子集，使得基 于这个特征子集训练出来的模型准确率最高。简单地说，特征选择就是保留有用 特征，移除冗余或无关的特征

子集搜索 一种直接的特征选择方法为子集搜索（Subset Search）。假设原始特征 数为d，则共有2d 个候选子集。特征选择的目标是选择一个最优的候选子集。最 暴力的做法是测试每个特征子集，看机器学习模型哪个子集上的准确率最高。但 是这种方式效率太低。常用的方法是采用贪心的策略：由空集合开始，每一轮添 加该轮最优的特征，称为前向搜索（Forward Search）；或者从原始特征集合开始， 每次删除最无用的特征，称为反向搜索（Backward Search）

## 5.2 特征抽取

特征抽取（Feature Extraction）是构造一个新的特征空间，并将原始特征投 影在新的空间中。以线性投影为例，原始特征向量x ∈ Rd，经过线性投影后得到 在新空间中的特征向量x′。

![](img/fea.png)

其中P ∈ R<sup>k×d</sup> 为映射矩阵

特征抽取又可以分为监督和无监督的方法。监督的特征学习的目标是抽取 对一个特定的预测任务最有用的特征，比如线性判别分析（Linear Discriminant Analysis，LDA）。而无监督的特征学习和具体任务无关，其目标通常是减少冗余信息和噪声，比如主成分分析（Principal Components Analysis，PCA）。

## 5.3 优点

**特征选择**和**特征抽取**的优点是可以用较少的特征来表示原始特征中的大部 分相关信息，去掉噪声信息，并进而提高计算效率和减小维度灾难（Curse Of Dimensionality）。对于很多没有正则化的模型，特征选择和特征抽取非常必要。 经过特征选择或特征抽取后，特征的数量一般会减少，因此特征选择和特征抽取也经常称为维数约减或降维（Dimension Reduction）。

# 6 模型评估

为了衡量一个机器学习模型的好坏，需要给定一个测试集，用模型对测试集 中的每一个样本进行预测，并根据预测结果计算评价分数

对于分类问题，常见的评价标准有正确率、准确率、召回率和F值等。
给定测试集T = (x<sup>1</sup>,y<sup>1</sup>),··· ,(x<sup>N</sup>,y<sup>N</sup>)，假设标签y(n) ∈{1,··· ,C}， 用学习好的模型f(x;θ)对测试集中的每一个样本进行预测，结果为Y = y'<sup>1</sup>,··· ,y'<sup>N</sup>。 

准确率 最常用的的评价指标为准确率（Accuracy)

![](img/acc.png)

错误率 和准确率相对应的就是错误率（Error Rate）。 

![](img/MS1BY2M=.png)

**查准率和查全率**:准确率是所有类别整体性能的平均，如果希望对每个类都进行性能估计，就需要计算查准率（Precision）和查全率（Recall）。查准率和查全率是 广泛用于信息检索和统计学分类领域的两个度量值，在机器学习的评价中也被 大量使用

对于类别c来说，模型在测试集上的结果可以分为以下四种情况:

1. 真正例（True Positive，TP）：一个样本的真实类别为c并且模型正确地预 测为类别c
2. 假负例（False Negative，FN）：一个样本的真实类别为c，模型错误地预测 为其它类
3. 假正例（False Positive，FP）一个样本的真实类别为其它类，模型错误地预 测为类c。
4. 真负例（True Negative，TN）：一个样本的真实类别为其它类，模型也预测 为其它类。这类样本数量记为TNc。对于类别c来说，这种情况一般不需要 关注

这四种情况的关系如表2.3所示的混淆矩阵来表示

![](img/hunxiao.png)

查准率（Precision），也叫精确度或精度，类别c的查准率为是所有预测为类 别c的样本中，预测正确的比例

![](img/precision.png)

查全率（Recall），也叫召回率，类别c的查全率为是所有真实标签为类别c 的样本中，预测正确的比例。

![](img/recall.png)

F值（F Measure）是一个综合指标，为查准率和查全率的调和平均。 

![](img/F.png)

***

# 7 机器学习相关理论和定理

当使用机器学习方法来解决某个特定问题时，通常靠经验或者多次试验来 选择合适的模型、训练样本数量以及学习算法收敛的速度等。但是经验判断或多 次试验往往成本比较高，也不太可靠，因此希望有一套理论能够分析问题难度、 计算模型能力，为学习算法提供理论保证，并指导机器学习模型和学习算法的 设计。这就是计算学习理论

## 7.1 PAC学习

机器学习中一个很关键的问题是期望错误和经验错误之间的差异，称为泛 化错误（Generalization Error）。泛化错误可以衡量一个机器学习模型f 是否可 以很好地泛化到未知数据。

![](img/gerror.png)

根据大数定律，当训练集大小|D|趋向于无穷大时，泛化错误趋向于0，即经 验风险趋近于期望风险

![](img/glimit.png)

由于我们不知道真实的数据分布p(x,y)，也不知道真实的目标函数g(x)， 因此期望从有限的训练样本上学习到一个期望错误为0的函数f(x)是不切实际 的。因此，需要降低对学习算法能力的期望，只要求学习算法可以以一定的概率 学习到一个近似正确的假设，即PAC学习。

> PCA 指出如果希望模型的假设空间越 大，泛化错误越小，其需要的样本数量越多。 

## 7.2 没有免费午餐定理

没有免费午餐定理（No Free Lunch Theorem，NFL）是由Wolpert和Macerday在最优化理论中提出的。没有免费午餐定理证明：对于基于迭代的最优化 算法，不存在某种算法对所有问题（有限的搜索空间内）都有效。如果一个算法 对某些问题有效，那么它一定在另外一些问题上比纯随机搜索算法更差。也就是 说，不能脱离具体问题来谈论算法的优劣，任何算法都有局限性。必须要“具体问 题具体分析”。

没有免费午餐定理对于机器学习算法也同样适用。不存在一种机器学习算 法适合于任何领域或任务。如果有人宣称自己的模型在所有问题上都好于其他 模型，那么他肯定是在吹牛

> 目前没有一个通用的机器学习算法

## 7.3 丑小鸭定理

[丑小鸭定理](https://www.douban.com/note/75396591/)（Ugly Duckling Theorem）是1969年由渡边慧提出的[Watan
渡边慧（Satosi Watanabe），“丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大”。这 个定理初看好像不符合常识，但是仔细思考后是非常有道理的。因为世界上不存在相似性的客观标准，一切相似性的标准都是主观的。如果以体型大小的角度 来看，丑小鸭和白天鹅的区别大于两只白天鹅的区别；但是如果以基因的角度来 看，丑小鸭与它父母的差别要小于他父母和其他白天鹅之间的差别

> 这里的“丑小鸭”是指白天鹅 的幼雏，而不是“丑陋的小鸭 子”。

丑小鸭定理的成立有着深刻的哲学背景，并与如下两个普遍原理有关：
（一）同维异维原理：两个事物总有相同属性，也有不同属性。从相同属性看过去，它们是一类；从不同属性看过去，它们是不同类。人们的价值观差异，主要体现在对不同属性重要性的差异上。例如生鸡蛋和煮熟的鸡蛋，外观属性一样但内在属性不同。画家看重外观属性，所以在他们眼里这两个鸡蛋是一类；餐馆里的食客看重内在属性，在他们眼里这两个鸡蛋就不是一类。
（二）（差异的）量变质变原理：随着对事物认知深度的增加，人们对事物的分辨力也随之提高。在低分辨力下相同的某个属性，在高分辨力下就可能不同。认知分辨率的提高会使差异化属性个数增多，无差异属性个数减少。即使每个属性的重要性不发生变化，其对事物差异的总体评价也可能不同，这是量变质变原理在差异认知上的反映。

## 7.4 奥卡姆剃刀

奥卡姆剃刀（Occam’s Razor）是由14世纪逻辑学家William of Occam提 出的一个解决问题的法则：“如无必要，勿增实体”。奥卡姆剃刀的思想和机器学 习上正则化思想十分类似：简单的模型泛化能力更好。如果有两个性能相近的模 型，我们应该选择更简单的模型。因此，在机器学习的学习准则上，我们经常会引 入参数正则化来限制模型能力，避免过拟合。

> 选择更简单的模型，泛化性能会更好

## 7.5 归纳偏置

在机器学习中，很多学习算法经常会对学习的问题做一些假设，这些假设就 称为归纳偏置（Inductive Bias） [Mitchell, 1997]。比如在最近邻分类器中，我们会 假设在特征空间中，一个小的局部区域中的大部分样本都同属一类。在朴素贝叶 斯分类器中，我们会假设每个特征的条件概率是互相独立的

归纳偏置在贝叶斯学习中也经常称为先验（Priors）。也就是说假设数据分布和我们的想象一致

# 相关阅读

倪云华: [奥卡姆剃刀思维：简单成就高效](https://baijiahao.baidu.com/s?id=1611383948703465038&wfr=spider&for=pc)

Ray:[丑小鸭定理及其推论的认知科学涵义-转](https://www.douban.com/note/75396591/)

hyc339408769: [梯度下降法小结](https://www.cnblogs.com/huangyc/p/9801261.html)

喂你在哪: [机器学习之特征选择和特征抽取](https://www.cnblogs.com/dyl222/p/11055756.html)
