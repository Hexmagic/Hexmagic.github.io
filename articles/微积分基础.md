- [写在前面](#%e5%86%99%e5%9c%a8%e5%89%8d%e9%9d%a2)
- [知识点](#%e7%9f%a5%e8%af%86%e7%82%b9)
- [函数](#%e5%87%bd%e6%95%b0)
- [导数](#%e5%af%bc%e6%95%b0)
- [微积分](#%e5%be%ae%e7%a7%af%e5%88%86)
- [偏导数](#%e5%81%8f%e5%af%bc%e6%95%b0)
- [凸函数定义](#%e5%87%b8%e5%87%bd%e6%95%b0%e5%ae%9a%e4%b9%89)
  - [凸函数的性质](#%e5%87%b8%e5%87%bd%e6%95%b0%e7%9a%84%e6%80%a7%e8%b4%a8)
- [泰勒展开式](#%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80%e5%bc%8f)
  - [泰勒公式的应用](#%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f%e7%9a%84%e5%ba%94%e7%94%a8)
  - [泰勒公式的推导](#%e6%b3%b0%e5%8b%92%e5%85%ac%e5%bc%8f%e7%9a%84%e6%8e%a8%e5%af%bc)
- [总结](#%e6%80%bb%e7%bb%93)
- [推荐阅读](#%e6%8e%a8%e8%8d%90%e9%98%85%e8%af%bb)
  
  ### 写在前面

首先声明我数学也不是太好，大学数学一直挂科来着，所以数学基础差的同学可以放心看了，文章有写的又不好的地方不足的地方记得指出了，如果这篇文章帮到你了，别忘点个赞，谢谢！

这里我只讲一些和机器学习有关的知识，尽量少写公式定理。讲的东西也会比较片面，如果你认为哪些东西对机器学习很重要而我又没有写，请指出来。

### 知识点

1. 导数和偏导数的定义与计算方法
2. 梯度向量的定义
3. 极值定理，可导函数在极值点处导数或梯度必须为0
4. 凸函数的定义与判断方法
5. 泰勒展开公式

### 函数

函数其实就是**定义域**到**值域**映射关系，通常用$f$代表，例如

$f(x)=2x+2$

至于函数的一些性质，我这里不再说了，毕竟大家都有脑子。

### 导数

这里我们着重讲导数，毕竟导数才是重中之重，函数用$f(x)$表示,导数(导函数)用$f^{'}(x)$表示，导数的导数用$f^{''}(x)$表示，依此类推。导数用微分法写作$\frac {dy} {dx}$

下面我们看一些常见的导数：

| 原函数        | 导函数                   |
| ---------- | --------------------- |
| $y=C(C为常数) | 0                     |
| $y=a^x$    | $y^{'}=a^xlna$        |
| $y=e^x$    | $y^{'}=e^x$           |
| $y=x^n     | $y^{'}=nx^{n-1}$      |
| $y=lnx$    | $y^{'}=\frac {1} {x}$ |

好了,其他的不写了,毕竟我们又不是要学习求导.

有人说等等，你写了这么多还没告诉我们导数到底是啥玩意啊？套用高中学的的知识，导数就是曲线上一点的斜率，如下图：
![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/25-15-15-06-dao.png)

**那么导数是如何算出来的呢？**

由高中的数学我们知道，曲线两点$(x_1,y_1),(x_2,y_2)$连线的斜率g如下

$g = \frac {f(x_1)-f(x_2)} {x_1-x_2}$
那么求曲线一点A$(x,y)$的斜率呢，我们可以拿这个点与离他很近的一个点M的连线的斜率去逼近，假设M与A的x轴差值为△x,那么M可以如下表示

$M=(x+△x,f(x+△x)$

很显然，两点连线的斜率如下

$g=\frac {f(x+△x)-f(x)} {(x+△x)-x}=\frac {f(x+△x)-f(x)} {△x}$

当△x无限小时，$g$的值无限逼近于A的实际斜率

好了导数你已经学会了，very good!

### 微积分

首先，微积分包括微分和积分，积分包括不定积分和定积分，我来把它串起来。先来看一张我百度过来的图
![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/25-15-15-18-jif.jpg)
简单来说假设下面黄色长方形的宽为△x,长方形的面积就是积分。当△x趋近于无穷小，全部长方形的面积(积分)就能逼近**曲线下的面积**。积分一般用∫表示。上面我们说到积分**有定积分**和**不定积分**，定积分就是确定上下限的积分，譬如$∫_1^2f(x)$就是求$f(x)$在1到2范围内的积分(曲线下的面积)。那不定积分是啥？可不是不确定上下限的积分，因为那通常没有意义。定积分求得是**原函数**(不知道原函数的自己面壁思过去),好了微积分就讲到这里

### 偏导数

上面讲过导数，现在我们来看一下它的进阶版本：**偏导数**，它一般记作$∂$。上面举的例子都是一元函数，类似$f(x)=a \cdot x+b$,偏导数一般指的是多元函数(我理解的)，例如：

$f(z)=a \cdot x+b \cdot y$

其中a,b是实数，x,y是变量，z是未知数。那么求x的偏导,就是按住y不动,求x的导数$\frac {∂f} {∂x}$(就是把y当成常数求导,推导到三元四元或者更多元函数中,就是把其他变量看作常数,求x的导数)
依次类推可以求y的偏导$\frac {∂f} {∂y}$.

接下来我们看一个机器学习中非常重要的数学概念:**梯度**,那么梯度是怎么来的呢?按照上面二元函数的例子,对该函数所有变量的求偏导,然后把它们写成一个向量**g**

$g=\binom {\frac {∂f} {∂x}} {\frac {∂f} {∂x}}$

这个向量**g**成为该函数的**梯度**。那么这个梯度有啥用呢，它一般运用于机器学习中求最小值的，因为沿负梯度方向函数值下降最快(这里先这么理解就好了，因为有些时候它并不是最快的)

### 凸函数定义

如果定义在某一区间上的一元实函数是连续函数，且对这一区间中的任何两点X1、X2，当X1<X2时，有不等式$f(q_1x_1+q_2x_2)\ge q_1f(x_1)+q_2f(x_2)$ 
其中q1、q2为正数，q1+q2=1，这时，我们把函数f(x)叫做**凸函数**。如果把上述条件中的“≥”改成“>”，则叫做**严格凸函数**

凸函数的概念是詹森(J.L.w.v.Jermen，1859～1925)引入的，他所采取的定义条件是

$f(\frac {x_1+x_2} {2})\ge \frac {f(x_1)+f(x_2)} {2}$

相当于上述定义中$q_1=q_2=\frac {1} {2}$ 的特殊情况形。这种定义对于连续函数来说是等价的。如下图左面师凸函数右面是凹函数.
![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1572438833903&di=37b4f9cce48e956e351c310ccae8e65b&imgtype=0&src=http%3A%2F%2Fwww.czbaa.com%2Fsites%2Fdefault%2Ffiles%2Ffield%2Fimage%2Fforum_img%2Ftu_ao_han_shu_01.png)

#### 凸函数的性质

1. 如果f(x)是凸函数，那么-f(x)即是凹函数，通常都是把凹函数转化为凸函数来研究。
2. 如果一元实函数f(x)在某区间二阶可导，那么这一函数为凸函数的充要条件是在这一区间上恒有$f^{''}(x)$大于等于0(对于严格凹函数，只要改成$f^{''}(x)>0$就可以了)。 [1] 
3. 在数学当中，凹函数是凸函数的相反

### 泰勒展开式

#### 泰勒公式的应用

泰勒公式，也称泰勒展开式。是用一个函数在某点的信息，描述其附近取值的公式。如果函数足够平滑，在已知函数在某一点的各阶导数值的情况下，泰勒公式可以利用这些导数值来做系数，构建一个多项式近似函数，求得在这一点的邻域中的值

简单来讲就是用一个多项式函数去逼近一个给定的函数(即尽量使多项式函数图像拟合给定的函数图像)，注意，逼近的时候一定是从函数图像上的某个点展开。如果一个非常复杂函数，想求其某点的值，直接求无法实现，这时候可以使用泰勒公式去近似的求该值，这是泰勒公式的应用之一。泰勒公式在机器学习中主要应用于梯度迭代

#### 泰勒公式的推导

首先看下泰勒公式:
$f(x)=f(a)+f^{'}(a)(x-a)+ \frac {f^{''}(a)} {2!} (x-a)^2+ \frac {f^{n}(a)} {n!}(x-a)^n+R_n(x)$

其中$R_n(x)$是误差项,我们先不理睬

首先考虑一个物理问题假设我们有起始位置两个作直线运动的点,相同时间内怎么使两者运行的距离相同?因为为两个点初始速度都为0,所以需要两者的速度增长率相同也就是加速度相同,而加速度还有个"加速度",复杂情况下加速的"加速度"还有"加速度",依次类推...,不过归根到底是一句话那就是:**只要两个事物的初始的速度相同，初速度随时间变化的情况相同，初速度随时间变化的变化相同，初速度随时间变化的变化的变化相同······如果推到极限，那么这两者的运动轨迹就会无限相同**

这是很直观的，我们模仿一个事物，想要模仿的越像，就不能仅仅模仿他的表面，还要模仿他的变化，如果他的变化都在变，我们就跟着变，只要每个细节都相同，那我们就真假莫辨。

我们都知道导数代表了就是当前的变化率，导数的导数就代表变化率的变化率，导数的导数的导数就是变化率的变化率的变化率······所以我们就知道，**我要找个东西来模仿原函数，让他俩各阶导数都相同**

好吧我们举个例子,我们用一个函数$g(x)$来模仿$y=e^x$,简单起见我们从$x=0$开始模仿,上面我们说过要初速度相同,也就是

$g(0)=f(0)=e^0=1$

得出

g(x)=1

画个图看看:

![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/30-18-45-31-ty1.png)

嗯,有点不太理想,不过别急我们还没有模仿"加速度"呢,也就是令$f^{''}(0)=g^{''}(0)=1$
那么有第二个版本的$g(x)$

$g(x)=1+x$

再画个图:
![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/30-18-45-34-ty2.png)
有那么点意思了,我们在来模仿"加速度"的"加速度",这里我们仍然有$f^{'''}(0)=g^{''}(0)=1$,那么有第三个版本:

$g(x)=1+x+\frac {1}{2} x^2$

如下图:
![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/30-18-48-05-t3.png)
看起来更好了,那么接下来又第四个版本:
![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/30-18-47-55-t4.png)
第五个版本:
![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/30-18-45-42-t5.png)
...

我们发现不管怎么模仿,在更远处仍有误差,毕竟世界上没有完全相同的两片叶子,这个误差就是上面提到的R(x)称为**余项**

上面我们用的从0开始模仿是泰勒展开式的一种特殊情况,称为**麦克劳林公式**:
几个常见函数的麦克劳林公式

$e^x=1+x+\frac{1} {2!} x^2+...\frac{1} {n} x^n$

$sin(x)=x-\frac{1}{3!}x^3+...+\frac{(-1)^{m-1}}{(2m-1)!}x^{2m-1}$
$cos(x)=1-\frac{1}{2}x^2+\frac{1}{3}x^3-...+\frac{(-1)^{n-1}}{n}x^n$

### 总结

还有很多东西没有讲，譬如多重积分，微分方程，泰勒级数等。为什么没有讲呢，以为文章是为了入门不会涉及太多的细节知识。欢迎留言补充,求赞！

### 推荐阅读

这里我尽量推荐一些比较容易理解，和快速阅读上手的书籍并尽量给出对应的pdf下载页面,其中普林斯顿微积分读本做深入了解用。

| 书籍名称       | 下载链接                                           | 推荐指数 |
| ---------- | ---------------------------------------------- | ---- |
| 漫画微积分 小岛宽之 | [下载地址](http://www.downcc.com/soft/258045.html) | ⭐⭐⭐  |
| 简单微积分 神永正博 | 买一本二手的                                         | ⭐⭐   |
| 普林斯顿微积分读本  | [下载地址](http://www.downcc.com/soft/320650.html) | ⭐    |
