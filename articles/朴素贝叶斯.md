## 朴素贝叶斯
![](https://upload-images.jianshu.io/upload_images/944794-f1f7f145da78effe.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分布方法。对于给定的训练数据集，首先基于特贝叶斯方法是以**贝叶斯原理**为基础，使用概率统计的知识对样本数据集进行分类。由于其有着坚实的数学基础，贝叶斯分类算法的误判率是很低的。贝叶斯方法的特点是结合先验概率和后验概率，即避免了只使用先验概率的主管偏见，也避免了单独使用样本信息的过拟合现象。贝叶斯分类算法在数据集较大的情况下表现出较高的准确率，同时算法本身也比较简单

朴素贝叶斯方法是在贝叶斯算法的基础上进行了相应的简化，即假定给定目标值时属性之间相互条件独立。也就是说没有哪个属性变量对于决策结果来说占有着较大的比重，也没有哪个属性变量对于决策结果占有着较小的比重。虽然这个简化方式在一定程度上降低了贝叶斯分类算法的分类效果，但是在实际的应用场景中，极大地简化了贝叶斯方法的复杂性

### 鉴瓜师
假定你是一名从事鉴定瓜好坏与否的鉴瓜师，老前辈给了我们一些数据，下面是部分数据

| 编号  | 色泽  | 根蒂  | 敲声  | 纹理  | 脐部  | 触感  | 好瓜  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1   | 青绿  | 蜷缩  | 浊响  | 清晰  | 凹陷  | 硬滑  | 是   |
| 2   | 乌黑  | 蜡缩  | 沉闷  | 清晰  | 凹陷  | 硬滑  | 是   |
| 3   | 青绿  | 稍蜷  | 浊响  | 清晰  | 稍凹  | 软粘  | 是   |
| 4   | 乌黑  | 稍蜷  | 浊响  | 稍糊  | 稍凹  | 软粘  | 是   |
| 5   | 乌黑  | 稍蜷  | 独响  | 清晰  | 稍凹  | 硬滑  | 是   |
| 6   | 乌黑  | 稍蜷  | 沉闷  | 稍糊  | 稍凹  | 硬滑  | 否   |
| 7  | 青绿  | 硬挺  | 清脆  | 清晰  | 平坦  | 软粘  | 否   |


...

我们需要从中学习一些数据来指导我们去买瓜(我们的目标必须是买好瓜啊)

在学习买瓜技巧之前我们要介绍两个数学概念(所以要学好数学啊，不然买瓜都可能被坑)：先验概率、后验概率。先验概率说的是如果我对这个西瓜没有任何了解，包括瓜的颜色、形状、瓜蒂是否脱落。按常理来说，西瓜成熟的概率大概是 80%。那么，这个概率 $P(瓜熟)$就被称为先验概率。也就是说，先验概率是根据以往经验和分析得到的概率，先验概率无需样本数据，不受任何条件的影响(实际学习任务中会按样本瓜熟出现的概率来确定这个概率，毕竟我们不可能吃遍所有瓜)。那后验概率是个啥鸡儿玩意儿?举个例子你前辈告诉瓜皮看起来纹理清晰的瓜是好瓜的概率大一些，这里**纹理清晰**是一个条件，在这个条件下会导致一个结果:这个瓜是好瓜的概率会变大，而这个变大后的概率就是**后验概率**，举个具体的例子本来一堆西瓜中有80%的西瓜是好瓜，也就是自己随便摸一个瓜有80%概率是好瓜。现在你根据dad的提示挑选一个**纹理清晰**的瓜A，现在这个是瓜是好瓜的概率就提升了，加入这个概率提升到了85%，这个85%也就是**后验概率**。假设前辈**纹理清晰**而且摸起来比较**硬**的瓜B，现在这个瓜B是好瓜的概率比A是好瓜的概率又有所提升了

综上所述，我们通过使用一系列限制条件来挑选瓜，通过多个条件筛选我们拿到好瓜的概率大大提升，好吧讲的有点跑偏了，我们现在讲的是贝叶斯，这里没有贝叶斯的影子啊，好那我们来给买瓜问题套上一个贝h叶斯的马甲，根据我们在大学学到的贝叶斯公式：

$P(B|A)= \frac {P(B)P(A|B)} {P(A)}$

**鉴瓜员**

我们提出这样一个问题, 现在西瓜的状态分成两种:好瓜和坏瓜，概率分别为 0.8 与 0.2，且好瓜里瓜皮纹理清晰的概率是 0.7，坏瓜里面瓜皮纹理的概率是 0.1。那么，如果我现在挑到了一个瓜皮纹理模糊的瓜，则该瓜是好瓜的概率多大，应用贝叶斯公式

$P(好瓜|纹理清晰)=\frac {P(好瓜)P(纹理清晰|好瓜)} {P(纹理清晰)} \; ;(1)$

已知:

* P(纹理清晰|好瓜)=0.7, 所以P(纹理模糊|好瓜)=0.3
* P(好瓜)) = 0.8
* P(纹理清晰|坏瓜)=0.1 ,所以P(纹理模糊|坏瓜)=0.9
* P(坏瓜) = 0.2

可是我们发现我们没有$P(纹理清晰)$这个东西，没关系通过全概率公式我们知道

$P(纹理清晰)=P(纹理清晰|好瓜)P(好瓜)  + P(纹理清晰|坏瓜)P(坏瓜)  \;;(2)$

所以上面的式子可以变成这样

$P(好瓜|纹理清晰)=\frac {P(好瓜)P(纹理模糊|好瓜)} {P(纹理模糊|好瓜)P(好瓜)  + P(纹理清晰|坏瓜)P(坏瓜)}=\frac {0.8 \cdot 0.3} {0.8 \cdot 0.3 + 0.2 \cdot 0.9}=0.428$ 

如下图:

* A代表P(纹理模糊|坏瓜),

* B代表P(纹理清晰|坏瓜)

* C代表P(纹理模糊|好瓜)

* D代表P(纹理清晰|好瓜)

![](https://upload-images.jianshu.io/upload_images/944794-62430b2f4cab5cc7.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

为什么只考录上图A和C呢，因为

![](https://upload-images.jianshu.io/upload_images/944794-64b86452b5f1063f.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

最终结果只有0.428，比乱挑一个的几率还低，看来我们应该听前辈的话，”不听老人言吃亏在眼前“,古人诚不欺我！好了，现在我们已经学会应用贝叶斯买瓜了(听前辈的建议选瓜)，你已经是个鉴瓜员了,能够根据一个特征来筛选好瓜了，加油，在朝向鉴瓜师的路上更进一步

**鉴瓜师**

为了提升我们的技术和逼格，我们决定学一些更高级操作。所以我们这里不仅同要通过纹理判断好瓜坏瓜，还要通过这个瓜的触感来提升我们的鉴瓜”技巧“，掌握了这项技巧你将成为一名**鉴瓜师**


这里我们又从前辈那里得知好瓜摸起比较硬滑的瓜占60%，坏瓜摸起来比较硬的占30%，好了 那么现在我们又一个纹理模糊摸起来比较硬的瓜，那么这个瓜是好瓜的概率是多少呢，根据上面的(1)式，得出

$P(好瓜|硬|纹理模糊)=\frac {P(好瓜)P(硬|好瓜)P(纹理模糊|好瓜)} {P(硬)P(纹理模糊)} = \frac {0.8 \cdot 0.6 \cdot 0.3 } {0.8 \cdot 0.3 \cdot 0.6+ 0.2 \cdot 0.9 \cdot 0.4} = 0.666$

看起来结果比之前0.428的好很多，那如果我们全部按照前辈的意见来买瓜呢

$P(好瓜|硬|纹理清晰)=\frac {P(好瓜)P(硬|好瓜)P(纹理清晰|好瓜)} {P(硬)P(纹理清晰)} = \frac {0.8 \cdot 0.6 \cdot 0.7 } {0.8 \cdot 0.7 \cdot 0.6+ 0.2 \cdot 0.1 \cdot 0.4} = 0.0.976$

看来好的东西我们就要接受学习。通过学习我们不仅掌握了更高端的技巧，而且现在给你一个瓜你可以根据它的纹理触感来判断该瓜是好瓜的概率的，现在使用的是两个特征进行判断，多个特征类似我这里不再推导。

朴素贝叶斯的原理就是通过上面的计算得出该瓜是好瓜和坏瓜的概率，哪个概率大我们就认定它是属于哪个分类。

上面提到**假定给定目标值时属性之间相互条件独立**，这是有道理的，想一想如果属性之间条件不独立，我们还能用上面的乘法那么简单的计算么，但是现实中往往不会那么理想，特征之间多少有点关联，所以使用朴素贝叶斯会丢失部分的准确性。

## 算法实现
输入：训练数据$T=\begin{Bmatrix}(x_1,y_1),(x_2,y_2),...(x_N,y_N) \end{Bmatrix}$,其中$x_i=(x_i^1,x_i^2,...,x_i^n)^T$,其中$x_i^j$是第i个样本第j个特征，$y\in \begin{Bmatrix}c_1,c_2,...c_k \end{Bmatrix}$
输出: 实例x的分类

1.计算先验概率及条件概率

$P(Y=c_k) = \frac {\sum_{i=1}^N(y_i=c_k)} {N}\;;(4)$

$P(X^j=a_{jl}|Y=c_k)=\frac {\sum_{i=1}^{N} I(x_i^j=a_{jl},y_i=c_k)} {\sum_{i=1}^{N}I(y_i=c_k)}\;;(5)$

$(4)$计算所有分类与总数据长度比例，也就是计算该类的先验概率
$(5)$计算各个特征下各个属性值得条件概率，其中分子是分类$k$下特征$j$值为$a_{jl}$的样本数量，分母为分类为$k$的样本数量

2.对于给定的实例计算后验概率
   
$P(Y=ck)\Pi P(X^j=x^j|Y=c_k)\;;(6)$

$(6)$是计算给定数据下后验概率，其中$\Pi$是连乘的意思

3.确定实例的类

$y = argmax_{ck} P(Y=ck)\Pi P(X^j=x^j|Y=c_k)\;;(7)$

$(7)$我们可以获取给定样本属于各个类的后验概率，我们去后验概率最大的类作为预测类

#### 代码实现
```python
from collections import defaultdict
import numpy as np

class Bayes(object):
    def __init__(self):
        self.p0 = 0 #负类的先验概率
        self.p1 = 0 #正类先验概率
        self.f0 = defaultdict(dict) #负类条件概率
        self.f1 = defaultdict(dict) #正类条件概率

    def calculate_pro(self, X, y):

        length = len(X)
        X1, X0 = [], []
        # 划分正负类
        for a, b in zip(X, y):
            if b == '1':
                X1.append(a)
            else:
                X0.append(a)
        X1, X0 = np.array(X1), np.array(X0)
        
        lenX1 = len(X1)
        lenX0 = len(X0)
        # 计算正负类先验概率
        self.p0 = lenX0 / length
        self.p1 = lenX1 / length

        # 统计正类条件概率
        for col in range(X1.shape[1]):
            col_val = X1[:, col]
            uniq_val = set(col_val)
            # 统计正类不同属性不同取值的条件概率
            for val in uniq_val:
                val_pro = len(col_val[col_val == val]) / lenX1
                self.f0[col][val] = val_pro
        # 统计负类条件概率
        for col in range(X0.shape[1]):
            col_val = X0[:, col]
            uniq_val = set(col_val)
            # 统计负类不同属性不同取值的条件概率
            for val in uniq_val:
                val_pro = len(col_val[col_val == val]) / lenX0
                self.f1[col][val] = val_pro

    def fit(self, X, y):
        self.calculate_pro(X, y)

    def predict(self, x):
        pro0 = self.p0
        pro1 = self.p1
        # 计算给定条件下正负类的后验概率
        for col, fe in enumerate(x):
            fe = str(fe)
            pro0 = pro0 * self.f1[col][fe]
            pro1 = pro1 * self.f0[col][fe]
        # 比较预测正负类下的后验概率，确定正负类
        if pro0 > pro1:
            return -1
        else:
            return 1


base = Bayes()
data = np.array(data)
data = [[1, 'S', -1], [1, 'M', -1], [1, 'M', 1], [1, 'S', 1], [1, 'S', -1],
        [2, 'S', -1], [2, 'M', -1], [2, 'M', 1], [2, 'L', 1], [2, 'L', 1],
        [3, 'L', 1], [3, 'M', 1], [3, 'M', 1], [3, 'L', 1], [3, 'L', -1]]
X, y = data[:, :-1], data[:, -1]
base.fit(X, y)
base.predict([2, 'S'])

```

## 优缺点

### 优点

朴素贝叶斯算法假设了数据集属性之间是相互独立的，因此算法的逻辑性十分简单，并且算法较为稳定，当数据呈现不同的特点时，朴素贝叶斯的分类性能不会有太大的差异。换句话说就是朴素贝叶斯算法的健壮性比较好，对于不同类型的数据集不会呈现出太大的差异性。当数据集属性之间的关系相对比较独立时，朴素贝叶斯分类算法会有较好的效果

### 缺点

属性独立性的条件同时也是朴素贝叶斯分类器的不足之处。数据集属性的独立性在很多情况下是很难满足的，因为数据集的属性之间往往都存在着相互关联，如果在分类过程中出现这种问题，会导致分类的效果大大降低

# 求赞求评论！
