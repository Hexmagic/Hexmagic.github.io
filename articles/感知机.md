### ### 感知机(preceptron)

感知机是二分类的线性分类模型，输入为特征空间，输出为实例的类别，值域为$\begin{Bmatrix}
 +1,&-1\\
\end{Bmatrix}$,感知机将对应输入空间中的一个分离超平面，属于判别模型。感知机的目的是求出将训练数据进行分割的超平面，原理是使用梯度下降对损失函数(误分类)进行极小化

### 感知机的数学模型

假设输入空间$X$属于$R^n$,输出空间$y \in \begin{Bmatrix}
 +1,&-1\\
\end{Bmatrix}$,输入$x\in X$，则

    $f(x)=sign(w \cdot x+b)$

称为感知机，其中$w$(权重)和$b$(偏置)为感知机模型参数，$sign$为符号函数

$sign(x) = \left\{\begin{matrix}
 +1&,x\geq 0  & \\ 
 -1&,x\lt 0  & 
\end{matrix}\right.$

### 感知机的几何解释

假设有如下线性方程：

$w\cdot x+b=0$

对应特征空间$R^n$的一个超平面$S$，其中$w$是超平面的[法向量]([https://baike.baidu.com/item/%E6%B3%95%E5%90%91%E9%87%8F/1161324?fr=aladdin](https://baike.baidu.com/item/%E6%B3%95%E5%90%91%E9%87%8F/1161324?fr=aladdin)，$b$是超平面的截距，这个超平面将将空间划分为两个部分，位于两侧的点对应两个类别，如下图

![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/14-20-03-43-test.png)其中点到平面的距离为$\frac {b} {||w||}$,其中$||w||$是2-范数，假设我们在平面属于$R^3$，根据点到平面的距离公式$d=\frac {ax+by+cz+b} {\sqrt {a^2+b^2+c^2}}$,代入原点(0,0,0)得到$d=\frac {b} {\sqrt {a^2+b^2+c^2}}\Rightarrow \frac{b}{||w||}$

> 最常用的范数是p-范数。若$x=[x_1,x_2,...x_n]$,则$||x||_p=(|x_1|^p+|x_2|^p+...|x_n|^p)^{\frac {1} {p}}$

### 感知机学习策略

#### 一、 数据集线性可分

感知机假设数据**线性可分**，给定一个数据集

$T=\begin{Bmatrix}(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\end{Bmatrix}$

若存在某个超平面$S$

$w \cdot x+b=0$

能够将数据集的正负例完全正确划分到超平面两侧，也就是对所有$y_i=+1$,有$w \cdot x+b>0$,对所有的$y_i=-1$,有$w\cdot x+b<0$,则称数据集**线性可分**

#### 二、学习策略

假设数据线性可分，感知机学习的目的是求得一个能将训练集完全正确分割个超平面(线性不可分会导致算法来会震荡)，为了达到目的我们需要顶一个一个损失函数。

损失函数的直觉选择是误分类点的个数，但是这样的损失函数**不是连续可导的函数**，难以优化，另一个选择是误分类点到超平面$S$的**总距离**，为此我们写出输入空间$R^n$中热议一点$x_i$到超平面$S$的距离

$d=\frac {1} {||w||}|w \cdot x_i+b|$

其次对于误分类的数据$(x_i,y_i)$来说有

$-y_i(w\cdot x_i+b)\gt 0$

利用上面的特性，可以把上面求距离的表达式中的绝对值符号去掉，得到

$d=-y_i\frac {1} {||w||}(w \cdot x_i+b)$

那么对于所有误分类点$M$有

$L（w,b)= -\frac{1} {||w||} \sum_{x_i\in M}y_i(w\cdot x +b)$

忽略$\frac {1} {||w||}$得到

$L(w,b)= -\sum_{x_i\in M}y_i(w\cdot x_i +b)$

感知学习的目的就是选择使损失函数最小化的$w$和$b$

### 感知机学习算法的原始形式

感知机学习算法是误分类驱动的，具体采用随机梯度下降，首先任意选择一个超平面参数$w_0$和$b_0$，然后才用随机梯度下降不断极小化损失函数

$L(w,b)= -\sum_{x_i\in M}y_i(w\cdot x_i +b)$

假设误分类集M,那么损失函数$L(w,b)$的梯度分别如下

$\triangledown_wL(w,b)= -\sum_{x_i \in M} y_i x_i$

$\triangledown_bL(w,b)= -\sum_{x_i \in M} y_i$

随机选择一个误分类点$(x_i,y_i)$,对$w$，$b$进行更新

$w \leftarrow w+ ηy_ix_i$

$b\leftarrow b + ηy_i$

其中$\eta$(读作eta)是学习率(learnig rate),综上所述得到如下算法:

输入: 训练集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$,  $y \in \begin{Bmatrix}+1,&-1\end{Bmatrix}$,学习率$\eta (0\lt \eta \leq1)$;

输出:$w,b$；感知机模型$f(x)=sing(w \cdot x + b)$

1. 选取初始值$w_0,b_0$

2. 选取数据$(x_i,y_i)$

3. 若$y_i(w \cdot x+b)<0$
   
   $w\leftarrow w+ \eta y_ix_i$
   
   $b\leftarrow b+\eta y_i$

4. 转至2直至训练集没有错误分类点

#### 原始形式的算法实现

假设$x_1=(1,1),x_2=(3,3),x_3=(4,3)$ 和$y_1=-1,y_2=+1,y_3=+1$，如下图

![](https://github.com/Hexmagic/store_image/blob/master/2019/10/15-11-23-52-QQ%E6%88%AA%E5%9B%BE20191015112246.png?raw=true)

实现代码如下

```python
import numpy as np
X = np.array([[1, 1], [3, 3], [4, 3]])
y = [-1, 1, 1]


class Perceptron(object):
    def __init__(self, learning_rate=1):
        self.w = np.array([0, 0]).reshape((-1, 1))
        self.b = 0

    def sign(self, x):
        return -1 if x < 0 else +1

    def calculate(self, X):
        yH = np.matmul(X, self.w) + self.b
        return np.apply_along_axis(self.sign, 1, yH)

    def get_wrong(self, X, yH, Y):
        for x, yh, y in zip(X, yH, Y):
            if yh != y:
                return {'x': x, 'y': y}
        return None

    def fit(self, X, y):
        while True:
            yH = self.calculate(X)
            wrong = self.get_wrong(X, yH, y)
            print(f"Wrong Point {wrong}")
            if not wrong:
                break
            self.w = self.w + (wrong['x'] * wrong['y']).reshape((-1, 1))
            self.b = self.b + wrong['y']
            print(f"update w to {self.w} update b to {self.b}")

per = Perceptron()
per.fit(X, y)
```

运行过程

| 迭代次数 | 误分类点  | $w$         | $b$ | $w \cdot x+b$ |
| ---- | ----- | ----------- | --- | ------------- |
| 0    |       | $(0,0)^T$   | 0   |               |
| 1    | $x_1$ | $(-1,-1)^T$ | 1   | $1$           |
| 2    | $x_2$ | $(2,2)^T$   | 0   | $2x^1+2x^2$   |
| 3    | $x_1$ | $(1,1)^T$   | -1  | $x^1+x^2-1$   |
| 4    | $x_1$ | $(0,0)^T$   | -2  | $-2$          |
| 5    | $x_2$ | $(3,3)^T$   | -1  | $3x^1+3x^2-1$ |
| 6    | $x_1$ | $(2,2)^T$   | -2  | $2x^1+2x^2-2$ |
| 7    | $x_1$ | $(1,1)^T$   | -3  | $x^1+x^2-3$   |
| 8    |       | $(1,1)^T$   | -3  | $x^1+x^2-3$   |

画出分割面

![](https://raw.githubusercontent.com/Hexmagic/store_image/master/2019/10/15-11-42-48-perceptron.png)
