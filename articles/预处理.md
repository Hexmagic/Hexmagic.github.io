现实世界的数据是“肮脏的”,导致这种情况的原因又很多

- [一、数据存在的问题](#%e4%b8%80%e6%95%b0%e6%8d%ae%e5%ad%98%e5%9c%a8%e7%9a%84%e9%97%ae%e9%a2%98)
- [二、 导入数据集](#%e4%ba%8c-%e5%af%bc%e5%85%a5%e6%95%b0%e6%8d%ae%e9%9b%86)
- [三、 处理缺失值](#%e4%b8%89-%e5%a4%84%e7%90%86%e7%bc%ba%e5%a4%b1%e5%80%bc)
- [四、 处理标签数据](#%e5%9b%9b-%e5%a4%84%e7%90%86%e6%a0%87%e7%ad%be%e6%95%b0%e6%8d%ae)
- [五、 属性缩放](#%e4%ba%94-%e5%b1%9e%e6%80%a7%e7%bc%a9%e6%94%be)
- [六、扩展阅读](#%e5%85%ad%e6%89%a9%e5%b1%95%e9%98%85%e8%af%bb)

## 一、数据存在的问题

* 不完整的：有些感兴趣的属性缺少属性值，或仅包含聚集数据
* 含噪声的：包含错误或者孤立点
* 不一致的：在编码或者命名上存在差异 等等
  ![](http://upload-images.jianshu.io/upload_images/944794-9dc8352ab63dbb6d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  没有高质量的数据，就没有高质量的挖掘结果,高质量的决策必须依赖高质量的数据,所以我们需要对手中不规则的数据进行预先过滤处理

## 二、 导入数据集

数据集一般是.csv的格式，csv文件以纯文本的格式保存表格化的数据，每一行都是一条记录，我们使用pandas自带的`read_csv`来加载csv文件使其转化为pandas中的`DataFrame`对象,然后我们可以通过对矩阵向量操作该对象

```python
dataset =  pd.read_csv('Data.csv')
X = dataset.iloc[:,:-1].values
Y = dataset.iloc[:,-1].values
# 等价于
X = dataset.values[:,:-1]
Y = dataset.values[:,:-1]
```

## 三、 处理缺失值

我们一般不能获得每条属性都完整的数据集,数据集会因为这样或者那样的原因缺失了某些属性值，如果不处理可能会导致模型学习的性能，我们可以用平均值或者中位数来替代缺失值，在这里我们使用scikit-learn提供的`Impute`来完成这项任务

```python
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)
imputer = imputer.fit(X[:, 1:3])
X[:, 1:3] = imputer.transform(X[:, 1:3])
```

> missing_values 会被替代,默认是NaN,可以自定义其他值，如 0

**💡**

Imputer 在0.20已经不再提倡使用,将在0.22移除,使用imputer.SimpleImputer 替代他

```python
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy="mean")
imp = imp.fit(X[:, 1:3])
X[:, 1:3] = imp.transform(X[:, 1:3])
```

**strategy选项**

| 值             | 含义  |
| ------------- | --- |
| mean          | 平均值 |
| median        | 中位数 |
| most_frequent | 众数  |

## 四、 处理标签数据

标签(Categorical)数据是指那些非数字的标签，一般这些标签的取值在一个有限固定大小的集合中,像是“是”和”否“这样的标签不能进行数学运算，所以不能直接被我们的模型使用，所以我们需要把它们转换为数字类型以便于我们的模型使用。为了达到这个目的我们使用scikit-learn的LabelEncoder来编码它

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_encoder_X = LabelEncoder()
X[:, 0] = label_encoder_X.fit_transform(X[:, 0])

#创建一个额外的变量
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
```

## 五、 属性缩放

大部分的机器学习算法在计算时使用欧几里得方法来计算两个数据点之间的距离(欧几里得距离),如果不对数据进行处理那些值比较大的属性会对距离的计算贡献的更多,为了填补这项缺陷我们可以把所有的属性值都归一化(最大值为1，最小值为0),这里我们使用StrandardScalar来达到这种效果

```python
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
```

**不同类型的预处理**

![sc.png](https://upload-images.jianshu.io/upload_images/944794-61ee90eb250d09fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在上图中，第一张图显示的是一个模拟的有两个特征的二分类数据集。第一个特征（x
轴）位于 10 到 15 之间。第二个特征（y 轴）大约位于 1 到 9 之间。接下来的 4 张图展示了 4 种数据变换方法，都生成了更加标准的范围。 scikit-learn 中的StandardScaler 确保每个特征的平均值为 0、方差为 1，使所有特征都位于同一量
级。但这种缩放不能保证特征任何特定的最大值和最小值。 RobustScaler 的工作原理与
StandardScaler 类似，确保每个特征的统计属性都位于同一范围。但 RobustScaler 使用的是中位数和四分位数 1，而不是平均值和方差。这样 RobustScaler 会忽略与其他点有很大不同的数据点（比如测量误差）。这些与众不同的数据点也叫异常值（outlier），可能会给其他缩放方法造成麻烦。与之相反， MinMaxScaler 移动数据，使所有特征都刚好位于 0 到 1 之间。对于二维数据集来说，所有的数据都包含在 x 轴 0 到 1 与 y 轴 0 到 1 组成的矩形中。

最后， Normalizer 用到一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向
量的欧式长度等于 1。换句话说，它将一个数据点投射到半径为 1 的圆上（对于更高维度
的情况，是球面）。这意味着每个数据点的缩放比例都不相同（乘以其长度的倒数）。如果
只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种
归一化

**数据缩放的威力**
这里我们使用乳腺癌数据进行分类,没有进行数据缩放前

```
from sklearn.svm import SVR,SVC
svm = SVC(C=100)
svm.fit(x_train,y_train)
print(svm.score(x_test,y_test))
```

评分为

```
0.6153
```

使用数据缩放对数据进行处理

```
scaler = StandardScaler()
scaler.fit(x_train)
x_train=scaler.transform(x_train)
x_test=scaler.transform(x_test)
svm = SVC()
svm.fit(x_train,y_train)
svm.score(x_test,y_test)
```

结果为

```
0.9650
```

> 测试和训练数据需要使用同一个缩放器进行缩放

## 六、扩展阅读
